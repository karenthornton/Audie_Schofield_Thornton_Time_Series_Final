---
title: "Startups Funding"
author: "Michael Audie, Karen Thornton, Hannah Schofield"
date: "2023-04-01"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#load packages
library(lubridate)
library(ggplot2)
library(forecast)
library(tseries)
library(smooth)
library(dplyr)
library(kableExtra)
```
#Background
-Clean Tech startups are on the rise 
-There are two stages of funding stages: early stage and late stage. Early stage can be considered seed funding and late stage can be something like series C funding. Late stage funding is often more money for fewer companies
-Companies that make it to late stage funding have a higher chance of being sucessfull.
-Greater late stage funding investment suggests a more maturation of the Clean Tech industry. 

#Research Questions
1. Are investments in clean tech startups expected to continue to increase?
2. As the clean tech industry matures will investments in late-stage startups outpace investments in earlier-stage startups? 

#Data
The data was collected from the IEA. The data is broken up into early and late stage investments. The data original went to 2022, but with Covid, the War in Ukraine, and a recession, there were dips in investment in both stages. Since we believe this doesn't reflect the trend of the industry, we decided to remove the year 2022. 

#Limitations
The dataset had some limitations that made it harder to predict future years. These limitations include, but are not limited to:
1. The data was yearly, as opposed to daily or monthly. This made it difficult to show seasonality. It also made the data less granular and will less data points it was harder to see a trend to model. 
2. As mentioned above, covid, and other factors like the war in Ukraine, and overall recession could be factors in the model that cannot be predicted or anticipated. 

#Data Wrangling - Full sets
```{r echo=FALSE, warning=FALSE}
#load the data
start_up_funding.raw<-read.csv("./start_ups_funding.csv")

#only USD for unit
full_funding.df<-start_up_funding.raw[start_up_funding.raw$unit=="USD",]

#creating a df for only the early stage
early_stage.df<-full_funding.df[full_funding.df$funding.stage=="Early stage",]

#aggregating the values into each year (early stages)
early_stage_sum.df<-aggregate(value~year, data = early_stage.df,sum)
#adding the first month of each year to the data (early stages)
early_stage_sum.df$year<-as.Date(paste0(early_stage_sum.df$year,"-01-01"))

#dropping last year from df
early_stage_sum.df<-early_stage_sum.df[1:22,]

#creating a df for only the late stage
late_stage.df<-full_funding.df[full_funding.df$funding.stage=="Later stage",]

#aggregating the values into each year (late stages)
late_stage_sum.df<-aggregate(value~year, data = late_stage.df,sum)
#adding the first month of each year to the data (late stages)
late_stage_sum.df$year<-as.Date(paste0(late_stage_sum.df$year,"-01-01"))
#dropping last year from df
late_stage_sum.df<-late_stage_sum.df[1:22,]
```

#Data Wrangling - Training Sets
```{r echo=FALSE}
#create an early stage df with the last year missing
early_stage_sum_training.df <- early_stage_sum.df %>%
  filter(between(year, as.Date("2000-01-01"), as.Date("2020-01-01")))

#create a late stage df with the last year missing
late_stage_sum_training.df <- late_stage_sum.df %>%
  filter(between(year, as.Date("2000-01-01"), as.Date("2020-01-01")))
```

#Creating Time Series
```{r echo=FALSE}
#create time series for full datasets
early.ts<-ts(early_stage_sum.df[,2], start = c(2000,1), frequency = 1)
late.ts<-ts(late_stage_sum.df[,2], start = c(2000,1), frequency = 1)

#create time series for the training datasets
early.training.ts<-ts(early_stage_sum_training.df[,2], start = c(2000,1), frequency = 1)
late.training.ts<-ts(late_stage_sum_training.df[,2], start = c(2000,1), frequency = 1)
```

#Data Wrangling - Testing Sets
```{r echo=FALSE}
#create an early stage testing df 2020-2021
early_stage_sum_testing.df <- early_stage_sum.df %>%
  filter(between(year, as.Date("2020-01-01"), as.Date("2021-01-01")))

#create a late stage testing df 2020-2021
late_stage_sum_testing.df <- late_stage_sum.df %>%
  filter(between(year, as.Date("2020-01-01"), as.Date("2021-01-01")))

early.test.ts<-ts(early_stage_sum_testing.df[,2], start = c(2020, 1), frequency = 1)
late.test.ts<-ts(late_stage_sum_testing.df[,2], start = c(2020, 1), frequency = 1)

autoplot(early.training.ts)
autoplot(early.test.ts)

autoplot(late.training.ts)
autoplot(late.test.ts)
```

#Visualization
```{r echo=FALSE}
#plot early and late full datasets
ggplot(early_stage_sum.df, aes(x=year))+
  geom_line(aes(y=early.ts,colour="Early"))+
  geom_line(aes(y=late.ts,colour="Later"))+
  scale_colour_manual(name="Stage of Funding", values =c("Early"="red","Later"="blue") )+
  theme_classic()+
  xlab("Year")+
  ylab("Startup Funding (USD)")
```

As you can see, both early and late stage funding has increased drastically over the years. Late stage passed early stage around 2016 and has been greater than early stage ever since. 

#Create ACFs and PACFs
```{r echo=FALSE}
#create ACFs and PACFs for the full datasets
par(mar=c(3,3,3,0));par(mfrow=c(1,2))
Acf(early.ts,lag.max = 40,main="Early ACF",ylim=c(-1,1))
Pacf(early.ts,lag.max = 40,main="Early PACF",ylim=c(-1,1))
Acf(late.ts,lag.max = 40,main="Late ACF",ylim=c(-1,1))
Pacf(late.ts,lag.max = 40,main="Late PACF",ylim=c(-1,1))
```

#Model Fit - Arima (Early)
```{r echo=FALSE}
#Fit the early data to an ARIMA
#Model 1: ARIMA
early.arima<-auto.arima(early.training.ts)
summary(early.arima)

early.arima.for<-forecast(early.arima, h=6)

#Visualize Model
autoplot(early.training.ts)+
  autolayer(early.arima.for, series = "ARIMA")+
  autolayer(early.test.ts, series = "Test")

ARIMA_scores <- accuracy(early.arima.for$mean, early.test.ts)
#store the performance metrics
```

#Model Fit - Mean, Naive, SSES (Early)
```{r echo=FALSE}
#Model 2: Arithmetic Mean 
early.meanSeas <- meanf(y = early.training.ts, h = 6)
mean_scores <- accuracy(early.meanSeas$mean, early.test.ts)  
#store the performance metrics

#Model 3: Seasonal Naive
early.snaiveSeas <- snaive(early.training.ts, h=6)
snaive_scores <- accuracy(early.snaiveSeas$mean, early.test.ts)  
#store the performance metrics

#Model 4: SSES
early.SSES <- es(early.training.ts, model="ZZZ", h=6, holdout=FALSE)
SSES_scores <- accuracy(early.SSES$forecast,early.test.ts)
#store the performance metrics

#Plot model + observed data
autoplot(early.ts) +
  autolayer(early.meanSeas, series="Mean",PI=FALSE)+
  autolayer(early.snaiveSeas, series="Naive",PI=FALSE)+
  autolayer(early.SSES$forecast, series="SSES",PI=FALSE)+
  autolayer(early.test.ts, series = "Test")+
  ylab("Funding")
```

#Model Fit - TBATS (Early)
```{r echo=FALSE}
#Model 5: TBATS
early.TBATS <-  tbats(early.training.ts)
early.TBATS.for <-  forecast(early.TBATS, h = 6)

#Plot forcasting results
autoplot(early.TBATS.for) + ylab("Funding")

#Plot model + observed data
autoplot(early.ts) +
  autolayer(early.TBATS.for, series="TBATS",PI=FALSE) +
  autolayer(early.test.ts, series = "Test")+
  ylab("Funding")

#model scoring
TBATS_scores <- accuracy(early.TBATS.for$mean,early.test.ts)
#store the performance metrics
```

#Model Fit - NN (Early)
```{r echo=FALSE}
#Model 6: Neural Network
early.NN.fit <- nnetar(early.training.ts,p=1,P=1)
  #nnetar(early.training.ts,p=1,P=0,xreg=fourier(early.training.ts, K=c(2,12)))

early.NN.for <- forecast(early.NN.fit, h=6)

#Plot forecasting results
autoplot(early.NN.for) + ylab("Funding")

#Plot model + observed data
autoplot(early.ts) +
  autolayer(early.NN.for, series="Neural Network",PI=FALSE)+
  autolayer(early.test.ts, series = "Test")+
  ylab("Funding")

# Model 3:  Neural Network 
NN_scores <- accuracy(early.NN.for$mean,early.test.ts)
#store performance metrics
```

#Model Plotting (Early)
```{r echo=FALSE}
#Early models plotting
autoplot(early.training.ts) +
  autolayer(early.arima.for, PI=FALSE, series="ARIMA") +
  autolayer(early.meanSeas, PI=FALSE, series="Mean") +
  autolayer(early.snaiveSeas, PI=FALSE, series="Naive") +
  autolayer(early.SSES$forecast, PI=FALSE, series="SSES") +
  autolayer(early.TBATS.for,PI=FALSE, series="TBATS") +
  autolayer(early.NN.for,PI=FALSE, series="NN") +
  autolayer(early.test.ts, series = "Test") +
  xlab("Year") + ylab("Funding ($)") +
  guides(colour=guide_legend(title="Forecast"))
```

#Model Scoring (Early)
```{r echo=FALSE}
#Early models comparison
early.scores <- as.data.frame(
  rbind(ARIMA_scores, mean_scores, snaive_scores, SSES_scores, TBATS_scores, NN_scores)
  )
row.names(early.scores) <- c("ARIMA", "Mean", "Naive", "SSES", "TBATS", "Neural Network")

print(early.scores)

#choose model with lowest RMSE
early.best_model_index <- which.min(early.scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(early.scores[early.best_model_index,]))  
```

#RMSE - Early
```{r echo=FALSE}
kbl(early.scores, 
      caption = "Forecast Accuracy for Early Stage Funding Date",
      digits = array(6,ncol(early.scores))) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(seas_scores[,"RMSE"]))
```


#Model Fit - ES (Late)
```{r echo=FALSE}
late.ES<-es(late.training.ts,model = "ZZZ",h=1,houldout=FALSE)
late.ES.forecast<-forecast(late.ES, h=2)

plot(late.ES.forecast)
autoplot(late.ts)+
  autolayer(late.ES.forecast$mean)
```

#Model Fit - Arima (Late)
```{r echo=FALSE}
#Fit the late data to an ARIMA
#Model 1: ARIMA
late.arima<-auto.arima(late.training.ts)
summary(late.arima)

late.arima.for<-forecast(late.arima, h=2)

#Visualize Model
autoplot(late.ts)+
  autolayer(late.arima.for$mean)+
  autolayer(late.test.ts)+
  ylab("Total Funding")

ARIMA_scores_late <- accuracy(late.arima.for$mean, late.test.ts)  
#store the performance metrics
```

#Model Fit - Mean, Naive, SSES (Late)
```{r echo=FALSE}
#Model 2: Arithmetic Mean 
late.meanSeas <- meanf(y = late.training.ts, h = 2)
mean_scores_late <- accuracy(late.meanSeas$mean, late.test.ts)  #store the performance metrics

#Model 3: Seasonal Naive
late.snaiveSeas <- snaive(late.training.ts, h=2)
snaive_scores_late <- accuracy(late.snaiveSeas$mean, late.test.ts)  #store the performance metrics

#Model 4: SSES
late.SSES <- es(late.training.ts, model="ZZZ", h=2, holdout=FALSE)
SSES_scores_late <- accuracy(late.SSES$forecast,late.test.ts)

#Plot model + observed data
autoplot(late.ts) +
  autolayer(late.meanSeas, series="Mean",PI=FALSE)+
  autolayer(late.snaiveSeas, series="Naive",PI=FALSE)+
  autolayer(late.SSES$forecast, series="SSES",PI=FALSE)+
  autolayer(late.test.ts, series = "Test")+
  ylab("Funding")
```

#Model Fit - TBATS (Late)
```{r echo=FALSE}
#Model 5: TBATS
late.TBATS <-  tbats(late.training.ts)
late.TBATS.for <-  forecast(late.TBATS, h = 2)

#Plot forcasting results
autoplot(late.TBATS.for) + ylab("Funding")

#Plot model + observed data
autoplot(late.ts) +
  autolayer(late.TBATS.for, series="TBATS",PI=FALSE) +
  autolayer(late.test.ts, series = "Test")+
  ylab("Funding")

#model scoring
TBATS_scores_late <- accuracy(late.TBATS.for$mean,late.test.ts)
#store scoring metrics
```

#Model Fit - NN (Late)
```{r echo=FALSE}
#Model 6: Neural Network
late.NN.fit <- nnetar(late.training.ts,p=1,P=1)
  #nnetar(early.training.ts,p=1,P=0,xreg=fourier(early.training.ts, K=c(2,12)))

late.NN.for <- forecast(late.NN.fit, h=2)

#Plot forecasting results
autoplot(late.NN.for) + ylab("Funding")

#Plot model + observed data
autoplot(late.ts) +
  autolayer(late.NN.for, series="Neural Network",PI=FALSE)+
  autolayer(late.test.ts, series = "Test")+
  ylab("Funding")

# Model 3:  Neural Network 
NN_scores_late <- accuracy(late.NN.for$mean,late.test.ts)
#store scoring metrics
```

#Model Plotting (Late)
```{r echo=FALSE}
#Early models plotting
autoplot(late.ts) +
  autolayer(late.arima.for, PI=FALSE, series="ARIMA") +
  autolayer(late.meanSeas, PI=FALSE, series="Mean") +
  autolayer(late.snaiveSeas, PI=FALSE, series="Naive") +
  autolayer(late.SSES$forecast, PI=FALSE, series="SSES") +
  autolayer(late.TBATS.for,PI=FALSE, series="TBATS") +
  autolayer(late.NN.for,PI=FALSE, series="NN") +
  xlab("Year") + ylab("Funding ($)") +
  guides(colour=guide_legend(title="Forecast"))
```

#Model Scoring (Late)
```{r echo=FALSE}
#Late models comparison
late.scores <- as.data.frame(
  rbind(ARIMA_scores_late, mean_scores_late, snaive_scores_late, SSES_scores_late, TBATS_scores_late, NN_scores_late)
  )
row.names(late.scores) <- c("ARIMA", "Mean", "Naive", "SSES", "TBATS", "Neural Network")

print(late.scores)

#choose model with lowest RMSE
late.best_model_index <- which.min(late.scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(late.scores[late.best_model_index,]))  
```

#RMSE - Late
```{r echo=FALSE}
kbl(late.scores, 
      caption = "Forecast Accuracy for Late Stage Funding Date",
      digits = array(6,ncol(late.scores))) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(seas_scores_late[,"RMSE"]))
```

#Plot the Best Models
```{r echo=FALSE}
#early
autoplot(early.ts) +
  autolayer(early.TBATS.for, series="TBATS",PI=FALSE)+
  autolayer(early.test.ts, series = "Test")+
  xlab("Year") + 
  ylab("Funding ($)") +
  guides(colour=guide_legend(title="Forecast"))

#late
autoplot(late.ts) +
  autolayer(late.TBATS.for, PI=FALSE, series="TBATS")+
  autolayer(late.test.ts, series = "Test")+
  xlab("Year") + 
  ylab("Funding ($)") +
  guides(colour=guide_legend(title="Forecast"))
```


#Conclusions
There are some limitations to this dataset which have made it difficult to predict. The data is yearly (as opposed to monthly or daily) which means less datapoints and less accuracy in predictions. The data shows a drastic dip in all investments following 2020, which could be attributed to the Covid pandemic. These limitations made predictions difficult. It is unclear whether there will continue to be an increase in clean tech startup investment in the future. Most models say yes to both early and late-stage, but the "best" model (according to RMSE) says no. This is the same case when comparing early and late-stage investments. Most models say yes, there will be more late-stage than early-stage investment, but the "best" model says no. 
