---
title: "Startups Funding"
author: "Michael Audie, Karen Thornton, Hannah Schofield"
date: "2023-04-01"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#load packages
library(lubridate)
library(ggplot2)
library(forecast)
library(tseries)
library(smooth)
library(dplyr)
library(kableExtra)
```

#Data Wrangling - Full sets
```{r echo=FALSE, warning=FALSE}
#load the data
start_up_funding.raw<-read.csv("./start_ups_funding.csv")

#only USD for unit
full_funding.df<-start_up_funding.raw[start_up_funding.raw$unit=="USD",]

#creating a df for only the early stage
early_stage.df<-full_funding.df[full_funding.df$funding.stage=="Early stage",]

#aggregating the values into each year (early stages)
early_stage_sum.df<-aggregate(value~year, data = early_stage.df,sum)
#adding the first month of each year to the data (early stages)
early_stage_sum.df$year<-as.Date(paste0(early_stage_sum.df$year,"-01-01"))

#creating a df for only the late stage
late_stage.df<-full_funding.df[full_funding.df$funding.stage=="Later stage",]

#aggregating the values into each year (late stages)
late_stage_sum.df<-aggregate(value~year, data = late_stage.df,sum)
#adding the first month of each year to the data (late stages)
late_stage_sum.df$year<-as.Date(paste0(late_stage_sum.df$year,"-01-01"))

```

#Data Wrangling - Training Sets
```{r echo=FALSE}
#create an early stage df with the last year missing
early_stage_sum_training.df <- early_stage_sum.df %>%
  filter(between(year, as.Date("2000-01-01"), as.Date("2021-01-01")))

#create a late stage df with the last year missing
late_stage_sum_training.df <- late_stage_sum.df %>%
  filter(between(year, as.Date("2000-01-01"), as.Date("2021-01-01")))
```

#Creating Time Series
```{r echo=FALSE}
#create time series for full datasets
early.ts<-ts(early_stage_sum.df[,2], start = c(2000,1), frequency = 1)
late.ts<-ts(late_stage_sum.df[,2], start = c(2000,1), frequency = 1)

#create time series for the training datasets
early.training.ts<-ts(early_stage_sum_training.df[,2], start = c(2000,1), frequency = 1)
late.training.ts<-ts(late_stage_sum_training.df[,2], start = c(2000,1), frequency = 1)
```

#Data Wrangling - Testing Sets
```{r echo=FALSE}
late.test.ts <- subset(late.ts, start = length(late.ts)-1)
early.test.ts <- subset(early.ts, start = length(early.ts)-1)

autoplot(early.training.ts)
autoplot(early.test.ts)

autoplot(late.training.ts)
autoplot(late.test.ts)

```

#Visualization
```{r echo=FALSE}
#plot early and late full datasets
ggplot(early_stage_sum.df, aes(x=year))+
  geom_line(aes(y=early.ts,colour="Early"))+
  geom_line(aes(y=late.ts,colour="Later"))+
  scale_colour_manual(name="Stage of Funding", values =c("Early"="red","Later"="blue") )+
  theme_classic()+
  xlab("Year")+
  ylab("Startup Funding (USD)")
```

#Create ACFs and PACFs
```{r echo=FALSE}
#create ACFs and PACFs for the full datasets
par(mar=c(3,3,3,0));par(mfrow=c(1,2))
Acf(early.ts,lag.max = 40,main="Early ACF",ylim=c(-1,1))
Pacf(early.ts,lag.max = 40,main="Early PACF",ylim=c(-1,1))
Acf(late.ts,lag.max = 40,main="Late ACF",ylim=c(-1,1))
Pacf(late.ts,lag.max = 40,main="Late PACF",ylim=c(-1,1))
```

#Model Fit - Arima (Early)
```{r echo=FALSE}
#Fit the early data to an ARIMA
#Model 1: ARIMA
early.arima<-auto.arima(early.training.ts)
summary(early.arima)

early.arima.for<-forecast(early.arima, h=2)

#Visualize Model
autoplot(early.ts)+
  autolayer(early.arima.for, series = "ARIMA")+
  autolayer(early.test.ts, series = "Test")

ARIMA_scores <- accuracy(early.arima.for$mean, early.test.ts)  #store the performance metrics

```

#Model Fit - Mean, Naive, SSES (Early)
```{r echo=FALSE}
#Model 2: Arithmetic Mean 
early.meanSeas <- meanf(y = early.training.ts, h = 2)
mean_scores <- accuracy(early.meanSeas$mean, early.test.ts)  #store the performance metrics

#Model 3: Seasonal Naive
early.snaiveSeas <- snaive(early.training.ts, h=2)
snaive_scores <- accuracy(early.snaiveSeas$mean, early.test.ts)  #store the performance metrics

#Model 4: SSES
early.SSES <- es(early.training.ts, model="ZZZ", h=2, holdout=FALSE)
SSES_scores <- accuracy(early.SSES$forecast,early.test.ts)

```
#Model Fit - TBATS (Early)
```{r echo=FALSE}
#Model 5: TBATS
early.TBATS <-  tbats(early.training.ts)
early.TBATS.for <-  forecast(early.TBATS, h = 2)

#Plot forcasting results
autoplot(early.TBATS.for) + ylab("Funding")

#Plot model + observed data
autoplot(early.ts) +
  autolayer(early.TBATS.for, series="TBATS",PI=FALSE) +
  autolayer(early.test.ts, series = "Test")+
  ylab("Funding")

#model scoring
TBATS_scores <- accuracy(early.TBATS.for$mean,early.test.ts)

```

#Model Fit - NN (Early)
```{r echo=FALSE}
#Model 6: Neural Network
early.NN.fit <- nnetar(early.training.ts,p=1,P=1)
  #nnetar(early.training.ts,p=1,P=0,xreg=fourier(early.training.ts, K=c(2,12)))

early.NN.for <- forecast(early.NN.fit, h=2)

#Plot forecasting results
autoplot(early.NN.for) + ylab("Funding")

#Plot model + observed data
autoplot(early.ts) +
  autolayer(early.NN.for, series="Neural Network",PI=FALSE)+
  autolayer(early.test.ts, series = "Test")+
  ylab("Funding")

# Model 3:  Neural Network 
NN_scores <- accuracy(early.NN.for$mean,early.test.ts)

```

#Model Plotting (Early)
```{r echo=FALSE}
#Early models plotting
autoplot(early.ts) +
  autolayer(early.arima.for, PI=FALSE, series="ARIMA") +
  autolayer(early.meanSeas, PI=FALSE, series="Mean") +
  autolayer(early.snaiveSeas, PI=FALSE, series="Naive") +
  autolayer(early.SSES$forecast, PI=FALSE, series="SSES") +
  autolayer(early.TBATS.for,PI=FALSE, series="TBATS") +
  autolayer(early.NN.for,PI=FALSE, series="NN") +
  xlab("Year") + ylab("Funding ($)") +
  guides(colour=guide_legend(title="Forecast"))

```

#Model Scoring (Early)
```{r echo=FALSE}
#Early models comparison
early.scores <- as.data.frame(
  rbind(ARIMA_scores, mean_scores, snaive_scores, SSES_scores, TBATS_scores, NN_scores)
  )
row.names(early.scores) <- c("ARIMA", "Mean", "Naive", "SSES", "TBATS", "Neural Network")

print(early.scores)

#choose model with lowest RMSE
early.best_model_index <- which.min(early.scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(early.scores[early.best_model_index,]))  

```

#RMSE - Early
```{r echo=FALSE}
kbl(early.scores, 
      caption = "Forecast Accuracy for Early Stage Funding Date",
      digits = array(6,ncol(early.scores))) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(seas_scores[,"RMSE"]))

```


#Model Fit - ES (Late)
```{r echo=FALSE}
late.ES<-es(late.training.ts,model = "ZZZ",h=1,houldout=FALSE)
late.ES.forecast<-forecast(late.ES, h=2)

plot(late.ES.forecast)
autoplot(late.ts)+
  autolayer(late.ES.forecast$mean)
```

#Model Fit - Arima (Late)
```{r echo=FALSE}
#Fit the late data to an ARIMA
#Model 1: ARIMA
late.arima<-auto.arima(late.training.ts)
summary(late.arima)

late.arima.for<-forecast(late.arima, h=2)

#Visualize Model
autoplot(late.ts)+
  autolayer(late.arima.for$mean)+
  autolayer(late.test.ts)+
  ylab("Total Funding")

ARIMA_scores_late <- accuracy(late.arima.for$mean, late.test.ts)  #store the performance metrics

```

#Model Fit - Mean, Naive, SSES (Late)
```{r echo=FALSE}
#Model 2: Arithmetic Mean 
late.meanSeas <- meanf(y = late.training.ts, h = 2)
mean_scores_late <- accuracy(late.meanSeas$mean, late.test.ts)  #store the performance metrics

#Model 3: Seasonal Naive
late.snaiveSeas <- snaive(late.training.ts, h=2)
snaive_scores_late <- accuracy(late.snaiveSeas$mean, late.test.ts)  #store the performance metrics

#Model 4: SSES
late.SSES <- es(late.training.ts, model="ZZZ", h=2, holdout=FALSE)
SSES_scores_late <- accuracy(late.SSES$forecast,late.test.ts)

```
#Model Fit - TBATS (Late)
```{r echo=FALSE}
#Model 5: TBATS
late.TBATS <-  tbats(late.training.ts)
late.TBATS.for <-  forecast(late.TBATS, h = 2)

#Plot forcasting results
autoplot(late.TBATS.for) + ylab("Funding")

#Plot model + observed data
autoplot(late.ts) +
  autolayer(late.TBATS.for, series="TBATS",PI=FALSE) +
  autolayer(late.test.ts, series = "Test")+
  ylab("Funding")

#model scoring
TBATS_scores_late <- accuracy(late.TBATS.for$mean,late.test.ts)

```

#Model Fit - NN (Late)
```{r echo=FALSE}
#Model 6: Neural Network
late.NN.fit <- nnetar(late.training.ts,p=1,P=1)
  #nnetar(early.training.ts,p=1,P=0,xreg=fourier(early.training.ts, K=c(2,12)))

late.NN.for <- forecast(late.NN.fit, h=2)

#Plot forecasting results
autoplot(late.NN.for) + ylab("Funding")

#Plot model + observed data
autoplot(late.ts) +
  autolayer(late.NN.for, series="Neural Network",PI=FALSE)+
  autolayer(late.test.ts, series = "Test")+
  ylab("Funding")

# Model 3:  Neural Network 
NN_scores_late <- accuracy(late.NN.for$mean,late.test.ts)

```

#Model Plotting (Late)
```{r echo=FALSE}
#Early models plotting
autoplot(late.ts) +
  autolayer(late.arima.for, PI=FALSE, series="ARIMA") +
  autolayer(late.meanSeas, PI=FALSE, series="Mean") +
  autolayer(late.snaiveSeas, PI=FALSE, series="Naive") +
  autolayer(late.SSES$forecast, PI=FALSE, series="SSES") +
  autolayer(late.TBATS.for,PI=FALSE, series="TBATS") +
  autolayer(late.NN.for,PI=FALSE, series="NN") +
  xlab("Year") + ylab("Funding ($)") +
  guides(colour=guide_legend(title="Forecast"))

```

#Model Scoring (Late)
```{r echo=FALSE}
#Late models comparison
late.scores <- as.data.frame(
  rbind(ARIMA_scores_late, mean_scores_late, snaive_scores_late, SSES_scores_late, TBATS_scores_late, NN_scores_late)
  )
row.names(late.scores) <- c("ARIMA", "Mean", "Naive", "SSES", "TBATS", "Neural Network")

print(late.scores)

#choose model with lowest RMSE
late.best_model_index <- which.min(late.scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(late.scores[late.best_model_index,]))  

```

#RMSE - Late
```{r echo=FALSE}
kbl(late.scores, 
      caption = "Forecast Accuracy for Late Stage Funding Date",
      digits = array(6,ncol(late.scores))) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(seas_scores_late[,"RMSE"]))

```

#Plot the Best Models
```{r echo=FALSE}
#early
autoplot(early.ts) +
  autolayer(early.NN.for, series="Neural Network",PI=FALSE)+
  autolayer(early.test.ts, series = "Test")+
  xlab("Year") + 
  ylab("Funding ($)") +
  guides(colour=guide_legend(title="Forecast"))

#late
autoplot(late.ts) +
  autolayer(late.meanSeas, PI=FALSE, series="Mean")+
  autolayer(late.test.ts, series = "Test")+
  xlab("Year") + 
  ylab("Funding ($)") +
  guides(colour=guide_legend(title="Forecast"))
```

#Try taking out bad years





#Data Wrangling - Training Sets 2
```{r echo=FALSE}
#create an early stage df with the last 2 years missing
early_stage_sum_training.df.good <- early_stage_sum.df %>%
  filter(between(year, as.Date("2000-01-01"), as.Date("2020-01-01")))

#create a late stage df with the last year missing
late_stage_sum_training.df.good <- late_stage_sum.df %>%
  filter(between(year, as.Date("2000-01-01"), as.Date("2020-01-01")))
```

#Creating Time Series
```{r echo=FALSE}

#create time series for the training datasets
early.training.ts.good<-ts(early_stage_sum_training.df.good[,2], start = c(2000,1), frequency = 1)
late.training.ts.good<-ts(late_stage_sum_training.df.good[,2], start = c(2000,1), frequency = 1)
```

#Data Wrangling - Testing Sets
```{r echo=FALSE}
late.test.ts.good <- subset(late.ts, start = length(late.ts)-1)
early.test.ts.good <- subset(early.ts, start = length(early.ts)-1)

autoplot(early.training.ts.good)
autoplot(early.test.ts)

autoplot(late.training.ts.good)
autoplot(late.test.ts)

```


#Model Fit - Arima (Early)
```{r echo=FALSE}
#Fit the early data to an ARIMA
#Model 1: ARIMA
early.arima.good<-auto.arima(early.training.ts.good)
summary(early.arima.good)

early.arima.for.good<-forecast(early.arima.good, h=3)

#Visualize Model
autoplot(early.ts)+
  autolayer(early.arima.for.good, series = "ARIMA")+
  autolayer(early.test.ts.good, series = "Test")

ARIMA_scores_good <- accuracy(early.arima.for.good$mean, early.test.ts.good)  #store the performance metrics

```

#Conclusions
#There are some limitations to this dataset which have made it difficult to predict. The data is yearly (as opposed to monthly or daily) which means less datapoints and less accuracy in predictions. The data shows a drastic dip in all investments following 2020, which could be attributed to the Covid pandemic. These limitations made predictions difficult. It is unclear whether there will continue to be an increase in clean tech startup investment in the future. Most models say yes to both early and late-stage, but the "best" model (according to RMSE) says no. This is the same case when comparing early and late-stage investments. Most models say yes, there will be more late-stage than early-stage investment, but the "best" model says no. 
